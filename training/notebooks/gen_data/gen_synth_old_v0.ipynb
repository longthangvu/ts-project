{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59721f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up one level to project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da77bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, math\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from util.config_util import dotdict\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8241ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality_base = 3.0\n",
    "w, m, a = seasonality_base*1, seasonality_base*2, seasonality_base*4\n",
    "\n",
    "hyperprior_params = dotdict({\n",
    "    # Seasonality parameters\n",
    "    'a_min': -a, 'a_max': a, 'a_fixed_variance': 0.15,\n",
    "    'm_min': -m, 'm_max': m, 'm_fixed_variance': 0.15,\n",
    "    'w_min': -w, 'w_max': w, 'w_fixed_variance': 0.15,\n",
    "    \n",
    "    # Trend parameters\n",
    "    'trend_lin_min': -0.015, 'trend_lin_max': 0.015, 'trend_lin_fixed_variance': 0.005,\n",
    "    'trend_exp_min': 1 - 0.005, 'trend_exp_max': 1 + 0.005, 'trend_exp_fixed_variance': 0.001,\n",
    "    'trend_exp_multiplier': 400,\n",
    "    \n",
    "    # Noise and resolution\n",
    "    'noise_k_min': 0.5, 'noise_k_max': 3.5,\n",
    "    'resolution_min': 0.1, 'resolution_max': 1.2, 'resolution_multiplier': 50,\n",
    "    \n",
    "    # Other parameters\n",
    "    'harmonics_min': 2, 'harmonics_max': 8,\n",
    "    'discreteness_min': 1, 'discreteness_max': 6,\n",
    "    'bias_zi_min': 0.8, 'bias_zi_max': 3.0,\n",
    "    'amplitude_min': 0.5, 'amplitude_max': 4.0,\n",
    "    'non_negative_prob': 0.2,\n",
    "    'offset_lin_min': -1.0, 'offset_lin_max': 1.2,\n",
    "    'offset_exp_min': -1.5, 'offset_exp_max': 2.0,\n",
    "    'f_zi_min': 0.0, 'f_zi_max': 0.6, 'f_zi_fixed_variance': 0.3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, glob\n",
    "from pathlib import Path\n",
    "\n",
    "n_sequence = 12_000\n",
    "out_dir   = \"series_bank/noise_v1_no_reject\"\n",
    "n_series  = 1_000_000\n",
    "shard_sz  = 20_000\n",
    "dtype     = np.float32\n",
    "seed      = 42\n",
    "device    = \"cpu\"\n",
    "\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "torch.manual_seed(seed); np.random.seed(seed)\n",
    "\n",
    "# snapshot meta for reproducibility\n",
    "meta = {\n",
    "    \"n_series\": n_series,\n",
    "    \"shard_size\": shard_sz,\n",
    "    \"dtype\": \"float32\",\n",
    "    \"seed\": seed,\n",
    "    \"n_sequence\": n_sequence,\n",
    "    \"hyperprior_params\": dict(hyperprior_params),\n",
    "}\n",
    "Path(out_dir, \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "n_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4583bca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_shards: 100%|██████████| 50/50 [7:28:19<00:00, 537.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected (quick std): 0\n",
      "Rejected (patch clip): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- strict constants ---\n",
    "L, H = 512, 96\n",
    "CLIP = 10.0\n",
    "STD_MIN = 0.02\n",
    "PATCH_S = 64\n",
    "MAX_RETRIES = 12\n",
    "n_series  = 1_000_000\n",
    "n_sequence = 12000\n",
    "\n",
    "num_shards = math.ceil(n_series / shard_sz)\n",
    "idx = 0\n",
    "rej_quick = rej_patch = 0\n",
    "\n",
    "for s in tqdm(range(num_shards), desc='num_shards'):\n",
    "    k = min(shard_sz, n_series - idx)\n",
    "    buf = np.empty((k, n_sequence), dtype=dtype)\n",
    "\n",
    "    for i in tqdm(range(k), desc='k', leave=False):\n",
    "        accepted = False\n",
    "        for tries in range(MAX_RETRIES):\n",
    "            # synth\n",
    "            y = gen_series()\n",
    "            y = y.contiguous().float()\n",
    "\n",
    "            # quick series-level gate on context region\n",
    "            # ctx_len = int(0.8 * n_sequence)\n",
    "            # y_ctx = y[:ctx_len]\n",
    "            # if y_ctx.std().item() < STD_MIN:\n",
    "            #     rej_quick += 1\n",
    "            #     continue\n",
    "\n",
    "            # subsampled patch-level saturation test\n",
    "            # if ctx_len - (L + H) > 0:\n",
    "            #     ends = torch.linspace(L-1, ctx_len-1-H, PATCH_S).long()\n",
    "            #     Xs = torch.stack([y[e-L+1:e+1] for e in ends], dim=0)\n",
    "            #     Zs = torch.stack([y[e+1:e+1+H] for e in ends], dim=0)\n",
    "            #     mu = Xs.mean()\n",
    "            #     med = Xs.median()\n",
    "            #     mad = (Xs - med).abs().median()\n",
    "            #     sigma = (1.4826 * mad).clamp_min(1e-9)   # no 0.10 floor here\n",
    "            #     Zn = torch.clamp((Zs - mu) / sigma, -CLIP, CLIP)\n",
    "            #     clip_frac = (Zn.abs() == CLIP).float().mean().item()\n",
    "            #     if clip_frac > 0.25:\n",
    "            #         rej_patch += 1\n",
    "            #         continue\n",
    "\n",
    "            # accepted = True\n",
    "            # break\n",
    "\n",
    "        # if not accepted:\n",
    "        #     # keep drawing until finite; skip stats gates, but require finiteness\n",
    "        #     while True:\n",
    "        #         y = gen_series()\n",
    "        #         y = y.contiguous().float()\n",
    "        #         if torch.isfinite(y).all():\n",
    "        #             break\n",
    "\n",
    "        buf[i] = y.cpu().numpy().astype(dtype)\n",
    "\n",
    "    np.save(Path(out_dir, f\"series_shard_{s:04d}.npy\"), buf)\n",
    "    idx += k\n",
    "\n",
    "print(f\"Rejected (quick std): {rej_quick}\")\n",
    "print(f\"Rejected (patch clip): {rej_patch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
